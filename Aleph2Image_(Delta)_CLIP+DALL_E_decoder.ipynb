{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Aleph2Image (Delta): CLIP+DALL-E decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit-19/PROJECT/blob/main/Aleph2Image_(Delta)_CLIP%2BDALL_E_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O78RfTZfh7ji"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Aleph2Image (Delta)\n",
        "\n",
        "\n",
        "> \"I was afraid that not a\n",
        "single thing on earth would ever again surprise me\"\n",
        "\n",
        "-Jorge Luis Borges\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aDcCMUnjQLh"
      },
      "source": [
        "\n",
        "#### What is this?\n",
        "\n",
        "This is a notebook that uses DALL-E's decoder and CLIP to generate images from text. I will very likely make this better & easier to use in the future.\n",
        "\n",
        "Feel free to send correspondence to [@advadnoun](https://twitter.com/advadnoun) on Twitter. If you use this notebook or modify it, please be cool and link back to my twitter. \n",
        "\n",
        "If you're feeling generous while your Waluigi (or whatever you choose) loads, you can donate to @rynnn on Venmo :)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKP0tnHaiTyl"
      },
      "source": [
        "#### Thanks!\n",
        "\n",
        "Many thanks to OpenAI for releasing their models CLIP and DALL-E (the encoder and decoder parts, specifically). I am not affiliated with them.\n",
        "\n",
        "\n",
        "https://github.com/openai/DALL-E/ (Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever)\n",
        "\n",
        "https://github.com/openai/CLIP (Alec Radford, \\* Jong Wook Kim,\\* Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
        "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever)\n",
        "\n",
        "\\* equal contribution\n",
        "\n",
        "Also, as a good launching point for future directions and to find more related work, see https://distill.pub/2017/feature-visualization/ by Chris Olah, Alexander Mordvintsev, Ludwig Schubert.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQULmg_lpMNq"
      },
      "source": [
        "# How do I use this?\n",
        "\n",
        "First, type in your description where it says *Insert text here*.\n",
        "\n",
        "\n",
        "Second, click the ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD0AAAA/CAYAAABTqsDiAAAEhklEQVRoBeWbx07rUBCG82xsYQtbeBB4CPYUsWAHQkiwAIEQsEGigwDRe++9M1efpYkOTmIf7NjXZfHLCY6P/Z0pZ4zHhZ+fH/n+/i7R19eXuPX5+SlZUAFoU+4JyCL4L2gT2IR1W/fj40P89P7+LklVEbocsMKagG6Qt7c3SZsc6ErACmuCmoCvr6+SRhXcwKZ1gQUSsJeXF3l6enL0+PgoqoeHB0mbitAaw0ADa4ICCNj9/b3c3d3J7e2to5ubGzF1fX0taZADbQKrZZ+fn2Vvby8TcodgERoLa7wCjFUzC61WJmkBDTCufHV1lW1oM46JX+L05OQk29CmlXHr8/PzzAAToiUxjXsDzQ618vHxsWxsbGQGvARaXZt1WGP54OBAVldXswuNlVmXgca1Ly4uHNilpaV8QFN4EM87OzsyPz+ffWiWKqDPzs5ke3s7H9DU1UCfnp7K1taWzM7OZtvSFCW5heZGQi09MzMTmaWHhoakr69PWltbHfGZv0VV9pYsWVqYYGkqMaA3Nzel2tATExPS0tIidXV1UlNTU1bs4zf8tpoTUAKt98xRQbPeA1IJtNLfOaZatUKs0FisoaHhz8A6ERxbDavHBs3FermygvltGSMseCzQuGUYC7sngrHCuHos0M3NzYFd2g2s3xkzaHKLHBpX1AuttG1sbJTa2lrf37mPD7qiRA5tY2XWZyYHeDeY1/eg1o4c2saCQOOqxKnNJOlEkNSCuHik0FRVeoFeW4VWgO7ubmt3D1K5RQpNOekFq/vc0MDbujvn0Mmy3UYKDYyCeW3LQQNg4+6VjvWagMRD+5WsiYMO695NTU2+npI49w6TyGxL1sQlMuLqr0uWnzubuYGxvWK30r5IY5qT2qy7xCXZ2sadTejEFic2ZSiwtu5sQie2DLW1tglj8zmolbmeyN2bk7De1tfX+2ZiG1h+w1iJv7UEHDe3SWp+4IzBWIwZVLFYWi+Oiw1jcY4NC8y1xArNCW1Ky3LWJobDuLRO/H+B1pNjMUC8XJ59/CZoltZzubcllo7r/97mhVBVUU6yXiM+B6m0zDG9PntCx/WEw+sCo9hnDZ35B3g85eAJh2npXEHzfDoXj2qxNA/lsbQ+lJ+bmwtcDEQRl2HGLBvTCm12IuQKOhc9J7RUsVab3UW7u7uysLCQXfd2Q19eXsr+/r4sLy9nH5pgZ9miX/vw8FDW1tayDY21tdmGFgzaJFm26CUbHx+XwcFB6enpEZ5EdHV1SWdnp3R0dEh7e3uJ2traJOkq0BtqtkqaXYPc5UxPTzvgw8PDMjAwIP39/U6t3NvbKyomJE1yoLUplmRGUywujrVpoltZWXF6yqampmRyctKZgLGxMRkdHXU0MjIiaVOxs1/vtrRzkIQGOJl8fX3dgV9cXHSyOiWqitvAtMl5Rcl0cU1oFCqAU6GR2KiIsDyiLdoUk5ImFd/LMsHVzQEnsQFP0UKPGcIDjo6OrMWkJUm/3sBTcDI5Ftf3OUhuOgHmK0i855FG/YLmxTQyuRYsCo/lmQDWcbdIfGnTPyMBsv9CTWMwAAAAAElFTkSuQmCC) \n",
        "button that shows up next to the text under the heading **This one!**. The button appears when you move your mouse over the text **This one!** Wait for a bit as it loads and goes in circles, and then move on to the third step when it finishes running.\n",
        "\n",
        "Third, click on the upper bar at the top of the page where it says **Runtime**, and then click **Restart and run all**. \n",
        "\n",
        "Your output will appear at the bottom of this page as it trains after a little while. Scroll down below everything else to see new images appear. The images will start by looking like dirt, but the page will eventually ding and show new images as it begins to attempt to match the image to your description. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3cYWzZkrbJd"
      },
      "source": [
        "# Choose Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XTXxdSqFJw_"
      },
      "source": [
        "text_input = '''a tree at sunset'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nD1n0xEBcko"
      },
      "source": [
        "# This one!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21a2bd5-f24e-481e-f74d-653b6769d86c"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.7.1+cu110 in /usr/local/lib/python3.7/dist-packages (1.7.1+cu110)\n",
            "Requirement already satisfied: torchvision==0.8.2+cu110 in /usr/local/lib/python3.7/dist-packages (0.8.2+cu110)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (5.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcixx9Z3XYH"
      },
      "source": [
        "# Top\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJOg9MY7khd"
      },
      "source": [
        "# don't use half of these lol\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "from IPython import display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import glob\n",
        "\n",
        "from google.colab import output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUOAL4UcytFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa39483-684e-496b-8a0e-14d204fd949d"
      },
      "source": [
        "# were you lucky today?\n",
        "\n",
        "!nvidia-smi -L\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-7d5dab33-75bc-2fa9-fb82-7393147d9492)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeMCHwDdHIcu"
      },
      "source": [
        "# Perceptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm3_VmxpAiB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d465fbd9-8888-48d7-ef17-2bab6f6dc3e8"
      },
      "source": [
        "\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "\n",
        "\n",
        "%cd /content/CLIP/\n",
        "\n",
        "!pip install ftfy\n",
        "\n",
        "import os\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "clip.available_models()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the model\n",
        "perceptor, preprocess = clip.load('ViT-B/32', jit=True)\n",
        "perceptor = perceptor.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'CLIP' already exists and is not an empty directory.\n",
            "/content/CLIP\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (5.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50', 'RN101', 'RN50x4', 'ViT-B/32']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw3cxCJZDw4w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw0KBLebMywW"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s"
      },
      "source": [
        "# probably don't mess with this unless you're changing generator size\n",
        "im_shape = [512, 512, 3]\n",
        "sideX, sideY, channels = im_shape\n",
        "batch_size = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlIUu0jK3S19"
      },
      "source": [
        "# Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCj1wr8pOqOe"
      },
      "source": [
        "def displ(img, pre_scaled=True):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "  if not pre_scaled:\n",
        "    img = scale(img, 48*4, 32*4)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(img))\n",
        "  return display.Image(str(3)+'.png')\n",
        "\n",
        "def gallery(array, ncols=2):\n",
        "    nindex, height, width, intensity = array.shape\n",
        "    nrows = nindex//ncols\n",
        "    assert nindex == nrows*ncols\n",
        "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
        "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols, intensity))\n",
        "    return result\n",
        "\n",
        "def card_padded(im, to_pad=3):\n",
        "  return np.pad(np.pad(np.pad(im, [[1,1], [1,1], [0,0]],constant_values=0), [[2,2], [2,2], [0,0]],constant_values=1),\n",
        "            [[to_pad,to_pad], [to_pad,to_pad], [0,0]],constant_values=0)\n",
        "\n",
        "def get_all(img):\n",
        "  img = np.transpose(img, (0,2,3,1))\n",
        "  cards = np.zeros((img.shape[0], sideX+12, sideY+12, 3))\n",
        "  for i in range(len(img)):\n",
        "    cards[i] = card_padded(img[i])\n",
        "  print(img.shape)\n",
        "  cards = gallery(cards)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(cards))\n",
        "  return display.Image(str(3)+'.png')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHB7BcR1zNLZ"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlDVVMvzMUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fee8f4-715e-4fba-8efa-8db946cbb020"
      },
      "source": [
        "import io\n",
        "import os, sys\n",
        "import requests\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "!pip install git+https://github.com/openai/DALL-E.git\n",
        "\n",
        "\n",
        "from dall_e import map_pixels, unmap_pixels, load_model\n",
        "target_image_size = sideX\n",
        "\n",
        "def preprocess(img):\n",
        "    s = min(img.size)\n",
        "    \n",
        "    if s < target_image_size:\n",
        "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
        "        \n",
        "    r = target_image_size / s\n",
        "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
        "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
        "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
        "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
        "    return map_pixels(img)\n",
        "\n",
        "\n",
        "model = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", 'cuda')\n",
        "encoder = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", 'cuda')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/DALL-E.git\n",
            "  Cloning https://github.com/openai/DALL-E.git to /tmp/pip-req-build-mdaw_0w6\n",
            "  Running command git clone -q https://github.com/openai/DALL-E.git /tmp/pip-req-build-mdaw_0w6\n",
            "Requirement already satisfied (use --upgrade to upgrade): DALL-E==0.1 from git+https://github.com/openai/DALL-E.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (7.1.2)\n",
            "Requirement already satisfied: blobfile in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (1.1.2)\n",
            "Requirement already satisfied: mypy in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (0.812)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (1.19.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (1.7.1+cu110)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from DALL-E==0.1) (0.8.2+cu110)\n",
            "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile->DALL-E==0.1) (3.0.12)\n",
            "Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.7/dist-packages (from blobfile->DALL-E==0.1) (3.10.1)\n",
            "Requirement already satisfied: urllib3~=1.25 in /usr/local/lib/python3.7/dist-packages (from blobfile->DALL-E==0.1) (1.26.4)\n",
            "Requirement already satisfied: xmltodict~=0.12.0 in /usr/local/lib/python3.7/dist-packages (from blobfile->DALL-E==0.1) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from mypy->DALL-E==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: typed-ast<1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mypy->DALL-E==0.1) (1.4.2)\n",
            "Requirement already satisfied: mypy-extensions<0.5.0,>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from mypy->DALL-E==0.1) (0.4.3)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (8.7.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (1.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (54.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->DALL-E==0.1) (20.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E==0.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E==0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->DALL-E==0.1) (3.0.4)\n",
            "Building wheels for collected packages: DALL-E\n",
            "  Building wheel for DALL-E (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DALL-E: filename=DALL_E-0.1-cp37-none-any.whl size=6000 sha256=ae3a68bb7d878af18032b29cd06126f0786b5aecb979078015b87e40c3c473c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g5b51uwu/wheels/e9/f5/e7/efa7ddb4c5899f6e6ffbbd112b8c7a030872274a5cba9ccf04\n",
            "Successfully built DALL-E\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdnzwtbW0fnR"
      },
      "source": [
        "oi = encoder(map_pixels(.2*torch.nn.functional.interpolate(torch.rand(1, 3, sideX//4, sideY//4), (sideX, sideY))).cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-aCuJ7SD7Oh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaocGDQXz3Zx"
      },
      "source": [
        "# Latent coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCh2D8Dt8Xd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "afb7ba48-1393-47d3-8356-d0cd0addae45"
      },
      "source": [
        "\n",
        "\n",
        "class Pars(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pars, self).__init__()\n",
        "\n",
        "\n",
        "        hots = torch.nn.functional.one_hot((torch.arange(0, 8192).to(torch.int64)), num_classes=8192)\n",
        "        rng = torch.zeros(batch_size, 64*64, 8192).uniform_()**torch.zeros(batch_size, 64*64, 8192).uniform_(.1,1)\n",
        "        for b in range(batch_size):\n",
        "          for i in range(64**2):\n",
        "            rng[b,i] = hots[[np.random.randint(8191)]]\n",
        "\n",
        "        rng = rng.permute(0, 2, 1)\n",
        "\n",
        "        self.normu = torch.nn.Parameter(rng.cuda().view(batch_size, 8192, 64, 64))\n",
        "\n",
        "        # ended up not doing static, as the brown seems to actually work better here\n",
        "\n",
        "        # self.normu = torch.nn.Parameter(oi.cuda().clone())\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "\n",
        "      \n",
        "      normu = torch.softmax(hadies*self.normu.reshape(batch_size, 8192//2, -1), dim=1).view(batch_size, 8192, 64, 64)\n",
        "      return normu\n",
        "\n",
        "\n",
        "lats = Pars().cuda()\n",
        "mapper = [lats.normu]\n",
        "optimizer = torch.optim.Adam([{'params': mapper, 'lr': .05}])\n",
        "eps = 0\n",
        "\n",
        "\n",
        "\n",
        "tx = clip.tokenize(text_input)\n",
        "t = perceptor.encode_text(tx.cuda()).detach().clone()\n",
        "\n",
        "\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "\n",
        "will_it = False\n",
        "hadies = 1.\n",
        "with torch.no_grad():\n",
        "  al = unmap_pixels(torch.sigmoid(model(lats()).cpu().float())).numpy()\n",
        "  for allls in al:\n",
        "    displ(allls[:3])\n",
        "    print('\\n')\n",
        "  # print(lats())\n",
        "  # print(lats().sum())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9074d30c10a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku0ClXRE1OJO"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnJZ2bVZ4M9w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjnUIFnbTpy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WztSrRF23Rqg"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EuUz-ICNKUr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "99454555-2b61-49fa-c05b-b24d01a229e1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def checkin(loss):\n",
        "  global hadies\n",
        "  print('''\n",
        "  ##########################################################\n",
        "  ''',\n",
        "        loss, ' (loss)\\n',itt)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    \n",
        "    al = unmap_pixels(torch.sigmoid(model(lats())[:, :3]).cpu().float()).numpy()\n",
        "    for allls in al:\n",
        "      displ(allls)\n",
        "      display.display(display.Image(str(3)+'.png'))\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "  # the people spoke and they love \"ding\"\n",
        "  output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
        "\n",
        "\n",
        "def ascend_txt():\n",
        "  out = unmap_pixels(torch.sigmoid(model(lats())[:, :3].float()))\n",
        "\n",
        "  \n",
        "\n",
        "  cutn = 64 # improves quality\n",
        "  p_s = []\n",
        "  for ch in range(cutn):\n",
        "    size = int(sideX*torch.zeros(1,).uniform_(.8, .99))#.normal_(mean=.8, std=.3).clip(.5, .98))\n",
        "    offsetx = torch.randint(0, sideX - size, ())\n",
        "    offsety = torch.randint(0, sideX - size, ())\n",
        "    apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
        "    apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear')\n",
        "    p_s.append(apper)\n",
        "  into = torch.cat(p_s, 0)\n",
        "  # into = torch.nn.functional.interpolate(out, (224,224), mode='bilinear')\n",
        "\n",
        "  into = into + .2 * random.random() * torch.randn_like(into)\n",
        "\n",
        "  into = nom((into))\n",
        "\n",
        "\n",
        "  iii = perceptor.encode_image(into)\n",
        "\n",
        "\n",
        "  lat_l = 0\n",
        "\n",
        "\n",
        "\n",
        "  return [lat_l, 10*-torch.cosine_similarity(t, iii).view(-1, batch_size).T.mean(1)]\n",
        "\n",
        "def train(i):\n",
        "  global hadies\n",
        "  loss1 = ascend_txt()\n",
        "  loss = loss1[0] + loss1[1]\n",
        "  loss = loss.mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  # hadies /= 1.01\n",
        "  # hadies = max(hadies, 1.5)\n",
        "\n",
        "  for g in optimizer.param_groups:\n",
        "    g['lr'] = g['lr']*1.005\n",
        "    \n",
        "  \n",
        "  if itt % 50 == 0:\n",
        "    # print('temp', hadies)\n",
        "    # print(g['lr'], 'lr')\n",
        "    checkin(loss1)\n",
        "\n",
        "\n",
        "itt = 0\n",
        "for asatreat in range(10000):\n",
        "  train(itt)\n",
        "  itt+=1\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc1dd1cd288c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mitt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masatreat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0mitt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dc1dd1cd288c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mglobal\u001b[0m \u001b[0mhadies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mascend_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dc1dd1cd288c>\u001b[0m in \u001b[0;36mascend_txt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mascend_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munmap_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'unmap_pixels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZItfLwQXOc74"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc7a1Cc_2uWl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZjVHE2f7VyY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYJECGeGeAM3"
      },
      "source": [
        "# Sharpie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rexmnafo71De"
      },
      "source": [
        "hadies = 130000000\n",
        "will_it = True\n",
        "\n",
        "with torch.no_grad():\n",
        "  lll = lats()\n",
        "  al = unmap_pixels(torch.sigmoid(model(lll)[:, :3]).cpu().float()).numpy()\n",
        "  for allls in al:\n",
        "    displ(allls)\n",
        "    print('\\n')\n",
        "\n",
        "  # print(lll)\n",
        "  # print(torch.sum(lll))\n",
        "  displ(torch.topk(lll.view(batch_size, 8192, 64, 64), k=3, dim=1)[0].cpu().numpy()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H81NMi9JaLSE"
      },
      "source": [
        "print(lats())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}